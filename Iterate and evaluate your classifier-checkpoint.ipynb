{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (r'C:\\Users\\Benjamin\\Downloads\\sentiment-labelled-sentences\\sentiment\\amazon_cells_labelled.txt')\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['review', 'message']\n",
    "\n",
    "keywords = ['great', 'good', 'love', 'excellent', 'pleased', 'fun']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "\n",
    "data = sms_raw[keywords]\n",
    "target = sms_raw['message']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[487,  13],\n",
       "       [405,  95]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive: 13 bad reviews identified as good\n",
    "False negative: 405 good reviews identified as bad \n",
    "Sensitivity: 95/500 19% good reviews identified as good\n",
    "Specificity: 487/500 97% bad reviews identified as bad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59, 0.6 , 0.61, 0.57, 0.58, 0.6 , 0.59, 0.55, 0.6 , 0.53])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores are relatively consistent. Classifier not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (r'C:\\Users\\Benjamin\\Downloads\\sentiment-labelled-sentences\\sentiment\\amazon_cells_labelled.txt')\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['review', 'message']\n",
    "\n",
    "keywords = ['great', 'love', 'excellent', 'fun']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "\n",
    "data = sms_raw[keywords]\n",
    "target = sms_raw['message']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[499,   1],\n",
       "       [444,  56]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive: 1 bad review identified as good\n",
    "False negative: 444 good reviews identified as bad \n",
    "Sensitivity: 56/500 11% good reviews identified as good\n",
    "Specificity: 499/500 99% bad reviews identified as bad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59, 0.55, 0.56, 0.54, 0.56, 0.55, 0.58, 0.56, 0.54, 0.52])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores still consistent showing no overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (r'C:\\Users\\Benjamin\\Downloads\\sentiment-labelled-sentences\\sentiment\\amazon_cells_labelled.txt')\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['review', 'message']\n",
    "\n",
    "keywords = ['great', 'love', 'excellent', 'good', 'nice', 'winner', 'fast' ]\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "\n",
    "data = sms_raw[keywords]\n",
    "target = sms_raw['message']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[487,  13],\n",
       "       [400, 100]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive: 13 bad reviews identified as good\n",
    "False negative: 400 good reviews identified as bad \n",
    "Sensitivity: 100/500 20% good reviews identified as good\n",
    "Specificity: 487/500 97% bad reviews identified as bad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59, 0.62, 0.61, 0.57, 0.57, 0.61, 0.6 , 0.55, 0.59, 0.53])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores fluntuate a little more than the previous two but still fairly consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (r'C:\\Users\\Benjamin\\Downloads\\sentiment-labelled-sentences\\sentiment\\amazon_cells_labelled.txt')\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['review', 'message']\n",
    "\n",
    "keywords = ['bad', 'never', 'poor', 'horrible', 'unreliable', 'wasted', 'slow' ]\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "\n",
    "data = sms_raw[keywords]\n",
    "target = sms_raw['message']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 20, 480],\n",
       "       [  2, 498]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive: 480 good reviews identified as bad\n",
    "False negative: 2 bad reviews identified as good \n",
    "Sensitivity: 498/500 99% bad reviews identified as bad\n",
    "Specificity: 20/500 4% good reviews identified as good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49, 0.51, 0.53, 0.53, 0.52, 0.5 , 0.54, 0.51, 0.53, 0.51])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very little flunctuation. classifier not overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (r'C:\\Users\\Benjamin\\Downloads\\sentiment-labelled-sentences\\sentiment\\amazon_cells_labelled.txt')\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['review', 'message']\n",
    "\n",
    "keywords = ['bad', 'poor', 'horrible', 'unreliable', 'slow', 'waste']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "\n",
    "data = sms_raw[keywords]\n",
    "target = sms_raw['message']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25, 475],\n",
       "       [  0, 500]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive: 475 good reviews identified as bad\n",
    "False negative: 0 bad reviews identified as good \n",
    "Sensitivity: 500/500 100% bad reviews identified as bad\n",
    "Specificity: 25/500 5% good reviews identified as good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52, 0.52, 0.55, 0.53, 0.51, 0.51, 0.53, 0.51, 0.55, 0.51])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest flunctuation of all the classifiers. No overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the classifiers seem to overfit after evaluating them. \n",
    "\n",
    "The last classifier seemed to perform the best. Using 'negative' words to identify bad reviews instead of the opposite worked better for the individual purpose. However the third classifier worked best for overall performance of identifying both.\n",
    "\n",
    "Negative features appear to be more inpactful on the performance of the classifier. Using negative words to find negative reviews works better than the opposite. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
